{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a2770b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdcedc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -q \"/content/drive/MyDrive/action-video.zip\" -d \"/content/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5005c162",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers evaluate decord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97324dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import evaluate\n",
    "import decord\n",
    "from decord import VideoReader, cpu\n",
    "import random\n",
    "import os\n",
    "\n",
    "decord.bridge.set_bridge('torch')\n",
    "\n",
    "# Config\n",
    "MODEL_CKPT = \"MCG-NJU/videomae-base-finetuned-kinetics\"\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 10 \n",
    "NUM_FRAMES = 16 \n",
    "LEARNING_RATE = 5e-5\n",
    "\n",
    "TRAIN_DIR = Path('action-video/data/data_train')\n",
    "TEST_DIR = Path('action-video/data/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55007d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoMAEDataset(Dataset):\n",
    "    def __init__(self, root_dir, image_processor, num_frames=16, is_train=True):\n",
    "        self.root = root_dir\n",
    "        self.image_processor = image_processor\n",
    "        self.num_frames = num_frames\n",
    "        self.is_train = is_train\n",
    "\n",
    "        self.classes = sorted([d.name for d in self.root.iterdir() if d.is_dir()])\n",
    "        self.class2id = {c: i for i, c in enumerate(self.classes)}\n",
    "        self.id2class = {i: c for c, i in self.class2id.items()}\n",
    "\n",
    "        self.samples = []\n",
    "        for cls in self.classes:\n",
    "            cls_path = self.root / cls\n",
    "            for vid_folder in sorted([d for d in cls_path.iterdir() if d.is_dir()]):\n",
    "                frames = sorted([str(p) for p in vid_folder.glob('*.jpg')])\n",
    "                if len(frames) > 0:\n",
    "                    self.samples.append((frames, self.class2id[cls]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def _get_indices(self, total_frames):\n",
    "        # Chọn 16 frame đều nhau\n",
    "        if total_frames <= self.num_frames:\n",
    "            # Nếu ít hơn 16, lặp lại\n",
    "            indices = np.arange(total_frames)\n",
    "            # Pad bằng frame cuối\n",
    "            pad = [indices[-1]] * (self.num_frames - total_frames)\n",
    "            indices = np.concatenate([indices, pad])\n",
    "        else:\n",
    "            # Lấy cách đều (Uniform sampling)\n",
    "            tick = total_frames / float(self.num_frames)\n",
    "            indices = np.array([int(tick / 2.0 + tick * x) for x in range(self.num_frames)])\n",
    "\n",
    "        return indices.astype(int)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frame_paths, label = self.samples[idx]\n",
    "\n",
    "        indices = self._get_indices(len(frame_paths))\n",
    "        video = []\n",
    "        import cv2\n",
    "        from PIL import Image\n",
    "\n",
    "        for i in indices:\n",
    "            img_path = frame_paths[int(i)]\n",
    "\n",
    "            try:\n",
    "                img = Image.open(img_path).convert(\"RGB\")\n",
    "                video.append(np.array(img))\n",
    "            except Exception as e:\n",
    "                print(f\"Lỗi đọc ảnh: {img_path} - {e}\")\n",
    "                # Nếu lỗi ảnh, thêm ảnh đen để không crash\n",
    "                video.append(np.zeros((224, 224, 3), dtype=np.uint8))\n",
    "\n",
    "        # Processor xử lý (Resize, Normalize, Crop...)\n",
    "        inputs = self.image_processor(list(video), return_tensors=\"pt\")\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": inputs[\"pixel_values\"].squeeze(), # [16, 3, 224, 224]\n",
    "            \"labels\": torch.tensor(label)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485631e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor = VideoMAEImageProcessor.from_pretrained(MODEL_CKPT)\n",
    "\n",
    "full_dataset = VideoMAEDataset(TRAIN_DIR, image_processor, num_frames=NUM_FRAMES)\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_ds, val_ds = random_split(full_dataset, [train_size, val_size],\n",
    "                                generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "print(f\"Train: {len(train_ds)}, Val: {len(val_ds)}\")\n",
    "print(f\"Classes: {len(full_dataset.classes)}\")\n",
    "\n",
    "# 4. Load Model VideoMAE\n",
    "# ignore_mismatched_sizes=True để thay đổi lớp Head cuối cùng (từ 400 class Kinetics sang 51 class HMDB)\n",
    "model = VideoMAEForVideoClassification.from_pretrained(\n",
    "    MODEL_CKPT,\n",
    "    label2id=full_dataset.class2id,\n",
    "    id2label=full_dataset.id2class,\n",
    "    ignore_mismatched_sizes=True,\n",
    "    num_frames=NUM_FRAMES\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfde0994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm tính toán metric\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)\n",
    "\n",
    "# Cấu hình Train\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"videomae-hmdb51\",\n",
    "    remove_unused_columns=False,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    save_total_limit=2,             \n",
    "    fp16=True,                      \n",
    "    gradient_accumulation_steps=4,  \n",
    "    dataloader_num_workers=2\n",
    ")\n",
    "\n",
    "# Khởi tạo Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=image_processor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=None \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50625a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Bắt đầu Fine-tuning VideoMAE...\")\n",
    "trainer.train()\n",
    "\n",
    "# Lưu model cuối cùng\n",
    "trainer.save_model(\"/content/drive/MyDrive/videomae_best_model\")\n",
    "print(\"Đã lưu model tốt nhất\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4117a040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model đã train\n",
    "trained_model = VideoMAEForVideoClassification.from_pretrained(\"/content/drive/MyDrive/videomae_best_model\").to(\"cuda\")\n",
    "trained_model.eval()\n",
    "\n",
    "# Dataset Test\n",
    "test_folders = sorted([d for d in TEST_DIR.iterdir() if d.is_dir()], key=lambda x: int(x.name))\n",
    "\n",
    "predictions = []\n",
    "\n",
    "print(\"Running Inference on Test Set...\")\n",
    "with torch.no_grad():\n",
    "    for vid_dir in tqdm(test_folders):\n",
    "        vid_id = int(vid_dir.name)\n",
    "        frames = sorted([str(p) for p in vid_dir.glob('*.jpg')])\n",
    "\n",
    "        if len(frames) == 0: continue\n",
    "\n",
    "        # 1. Preprocess \n",
    "        indices = full_dataset._get_indices(len(frames))\n",
    "        video = []\n",
    "        from PIL import Image\n",
    "        for i in indices:\n",
    "            img = Image.open(frames[i]).convert(\"RGB\")\n",
    "            video.append(np.array(img))\n",
    "\n",
    "        inputs = image_processor(list(video), return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "\n",
    "        # 2. Predict\n",
    "        logits = trained_model(**inputs).logits\n",
    "        pred_label_idx = logits.argmax(-1).item()\n",
    "        pred_class = full_dataset.id2class[pred_label_idx]\n",
    "\n",
    "        predictions.append((vid_id, pred_class))\n",
    "\n",
    "with open('submission.csv', 'w') as f:\n",
    "    f.write('id,class\\n')\n",
    "    for vid, cls in sorted(predictions, key=lambda x: x[0]):\n",
    "        f.write(f'{vid},{cls}\\n')\n",
    "\n",
    "print(\"Submission saved!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
